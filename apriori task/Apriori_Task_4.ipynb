{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1422b500",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Implementation Assignment\n",
    "\n",
    "### Objective:\n",
    "You will implement the **Apriori algorithm** from scratch (i.e., without using any libraries like `mlxtend`) to find frequent itemsets and generate association rules.\n",
    "\n",
    "### Dataset:\n",
    "Use the [Online Retail Dataset](https://www.kaggle.com/datasets/vijayuv/onlineretail) from Kaggle. You can filter it for a specific country (e.g., `United Kingdom`) and time range to reduce size if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85128a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "- Remove rows with missing values\n",
    "- Filter out rows where `Quantity <= 0`\n",
    "- Convert Data into Basket Format\n",
    "\n",
    "ðŸ‘‰ **Implement code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c39ca-065e-4990-810b-75b7fa9a76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\OnlineRetail.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['Quantity'] > 0]\n",
    "basket = (df.groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "            .sum()\n",
    "            .unstack()\n",
    "            .fillna(0)\n",
    "            .gt(0) \n",
    "            .astype(int)) \n",
    "\n",
    "print(basket.head())\n",
    "+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37baf6",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm\n",
    "Step-by-Step Procedure:\n",
    "1. Generate Frequent 1-Itemsets\n",
    "Count the frequency (support) of each individual item in the dataset.\n",
    "Keep only those with support â‰¥ min_support.\n",
    "â†’ Result is L1 (frequent 1-itemsets)\n",
    "2. Iterative Candidate Generation (k = 2 to n)\n",
    "While L(k-1) is not empty:\n",
    "a. Candidate Generation\n",
    "\n",
    "Generate candidate itemsets Ck of size k from L(k-1) using the Apriori property:\n",
    "Any (k-itemset) is only frequent if all of its (kâˆ’1)-subsets are frequent.\n",
    "b. Prune Candidates\n",
    "Eliminate candidates that have any (kâˆ’1)-subset not in L(k-1).\n",
    "c. Count Support\n",
    "For each transaction, count how many times each candidate in Ck appears.\n",
    "d. Generate Frequent Itemsets\n",
    "Form Lk by keeping candidates from Ck that meet the min_support.\n",
    "Repeat until Lk becomes empty.\n",
    "Implement the following functions:\n",
    "1. `get_frequent_itemsets(transactions, min_support)` - Returns frequent itemsets and their support\n",
    "2. `generate_candidates(prev_frequent_itemsets, k)` - Generates candidate itemsets of length `k`\n",
    "3. `calculate_support(transactions, candidates)` - Calculates the support count for each candidate\n",
    "\n",
    "**Write reusable functions** for each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f9310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def calculate_support(transactions, candidates):\n",
    "   \n",
    "    support_count = {candidate: 0 for candidate in candidates}\n",
    "    for transaction in transactions:\n",
    "        transaction_set = set(transaction)\n",
    "        for candidate in candidates:\n",
    "            if candidate.issubset(transaction_set):\n",
    "                support_count[candidate] += 1\n",
    "    return support_count\n",
    "\n",
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    \n",
    "    candidates = set()\n",
    "    prev_frequent_list = list(prev_frequent_itemsets)\n",
    "    \n",
    "    for i in range(len(prev_frequent_list)):\n",
    "        for j in range(i + 1, len(prev_frequent_list)):\n",
    "            union_set = prev_frequent_list[i] | prev_frequent_list[j]\n",
    "            if len(union_set) == k:\n",
    "                # Prune: check all subsets of size k-1 are frequent\n",
    "                subsets = combinations(union_set, k - 1)\n",
    "                if all(frozenset(subset) in prev_frequent_itemsets for subset in subsets):\n",
    "                    candidates.add(union_set)\n",
    "    return candidates\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support):\n",
    "    \n",
    "    single_items = set()\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            single_items.add(frozenset([item]))\n",
    "\n",
    "    support_data = {}\n",
    "    current_frequent_itemsets = {\n",
    "        item for item, count in calculate_support(transactions, single_items).items()\n",
    "        if count >= min_support\n",
    "    }\n",
    "\n",
    "    for item, count in calculate_support(transactions, single_items).items():\n",
    "        if count >= min_support:\n",
    "            support_data[item] = count\n",
    "\n",
    "    all_frequent_itemsets = dict(support_data)  # store results\n",
    "\n",
    "    k = 2\n",
    "    while current_frequent_itemsets:\n",
    "        candidates = generate_candidates(current_frequent_itemsets, k)\n",
    "        candidate_support = calculate_support(transactions, candidates)\n",
    "        \n",
    "        current_frequent_itemsets = {\n",
    "            item for item, count in candidate_support.items()\n",
    "            if count >= min_support\n",
    "        }\n",
    "        \n",
    "        for item, count in candidate_support.items():\n",
    "            if count >= min_support:\n",
    "                support_data[item] = count\n",
    "                all_frequent_itemsets[item] = count\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c77548-21d5-4950-93ae-1eb84002083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets (support â‰¥ 2):\n",
      "{'bread'}: 4\n",
      "{'milk'}: 4\n",
      "{'diaper'}: 4\n",
      "{'beer'}: 3\n",
      "{'beer', 'diaper'}: 3\n",
      "{'milk', 'bread'}: 3\n",
      "{'bread', 'diaper'}: 3\n",
      "{'milk', 'diaper'}: 3\n",
      "{'cola'}: 2\n",
      "{'cola', 'diaper'}: 2\n",
      "{'cola', 'milk'}: 2\n",
      "{'beer', 'bread'}: 2\n",
      "{'milk', 'beer'}: 2\n",
      "{'milk', 'bread', 'diaper'}: 2\n",
      "{'cola', 'diaper', 'milk'}: 2\n",
      "{'milk', 'beer', 'diaper'}: 2\n",
      "{'beer', 'bread', 'diaper'}: 2\n"
     ]
    }
   ],
   "source": [
    "transactions = [\n",
    "    [\"bread\", \"milk\"],\n",
    "    [\"bread\", \"diaper\", \"beer\", \"egg\"],\n",
    "    [\"milk\", \"diaper\", \"beer\", \"cola\"],\n",
    "    [\"bread\", \"milk\", \"diaper\", \"beer\"],\n",
    "    [\"bread\", \"milk\", \"diaper\", \"cola\"]\n",
    "]\n",
    "\n",
    "min_support = 2\n",
    "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
    "\n",
    "print(\"Frequent Itemsets (support â‰¥ 2):\")\n",
    "for itemset, support in sorted(frequent_itemsets.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print(f\"{set(itemset)}: {support}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c0fe",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules\n",
    "\n",
    "- Use frequent itemsets to generate association rules\n",
    "- For each rule `A => B`, calculate:\n",
    "  - **Support**\n",
    "  - **Confidence**\n",
    "- Only return rules that meet a minimum confidence threshold (e.g., 0.5)\n",
    "\n",
    "ðŸ‘‰ **Implement rule generation function below**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf26889",
   "metadata": {},
   "source": [
    "## Step 4: Output and Visualize\n",
    "\n",
    "- Print top 10 frequent itemsets\n",
    "- Print top 10 association rules (by confidence or lift)\n",
    "\n",
    "ðŸ‘‰ **Output results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9da87c-424c-4446-ae9e-eeb65efa8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rule': \"('bread',) => ('milk',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('milk',) => ('bread',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('bread',) => ('diaper',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('diaper',) => ('bread',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('milk',) => ('diaper',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('diaper',) => ('milk',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('diaper',) => ('beer',)\", 'Support': 0.6, 'Confidence': 0.75}\n",
      "{'Rule': \"('beer',) => ('diaper',)\", 'Support': 0.6, 'Confidence': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, min_confidence, transactions_count):\n",
    "    rules = []\n",
    "    for itemset, support in frequent_itemsets.items():\n",
    "        if len(itemset) >= 2:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for left in combinations(itemset, i):\n",
    "                    right = tuple(sorted(set(itemset) - set(left)))\n",
    "                    left_support = frequent_itemsets.get(tuple(sorted(left)), 0)\n",
    "                    if left_support > 0:\n",
    "                        confidence = support / left_support\n",
    "                        if confidence >= min_confidence:\n",
    "                            rules.append({\n",
    "                                'Rule': f\"{left} => {right}\",\n",
    "                                'Support': round(support, 2),\n",
    "                                'Confidence': round(confidence, 2)\n",
    "                            })\n",
    "    return rules\n",
    "\n",
    "frequent_itemsets = {\n",
    "    ('bread',): 0.8,\n",
    "    ('milk',): 0.8,\n",
    "    ('diaper',): 0.8,\n",
    "    ('beer',): 0.6,\n",
    "    ('bread', 'milk'): 0.6,\n",
    "    ('bread', 'diaper'): 0.6,\n",
    "    ('milk', 'diaper'): 0.6,\n",
    "    ('diaper', 'beer'): 0.6\n",
    "}\n",
    "\n",
    "rules = generate_association_rules(frequent_itemsets, 0.5, len(transactions))\n",
    "\n",
    "for rule in rules:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3443a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Frequent Itemsets:\n",
      "{'bread'}: 1.000\n",
      "{'milk'}: 0.600\n",
      "{'butter'}: 0.600\n",
      "{'milk', 'bread'}: 0.600\n",
      "{'bread', 'butter'}: 0.600\n",
      "{'eggs'}: 0.400\n",
      "{'eggs', 'bread'}: 0.400\n",
      "{'milk', 'butter'}: 0.400\n",
      "{'milk', 'bread', 'butter'}: 0.400\n",
      "\n",
      "Top 10 Association Rules (sorted by confidence):\n",
      "{'eggs'} => {'bread'} | Support: 0.400, Confidence: 1.000\n",
      "{'milk'} => {'bread'} | Support: 0.600, Confidence: 1.000\n",
      "{'butter'} => {'bread'} | Support: 0.600, Confidence: 1.000\n",
      "{'milk', 'butter'} => {'bread'} | Support: 0.400, Confidence: 1.000\n",
      "{'milk'} => {'butter'} | Support: 0.400, Confidence: 0.667\n",
      "{'butter'} => {'milk'} | Support: 0.400, Confidence: 0.667\n",
      "{'milk'} => {'bread', 'butter'} | Support: 0.400, Confidence: 0.667\n",
      "{'butter'} => {'milk', 'bread'} | Support: 0.400, Confidence: 0.667\n",
      "{'milk', 'bread'} => {'butter'} | Support: 0.400, Confidence: 0.667\n",
      "{'bread', 'butter'} => {'milk'} | Support: 0.400, Confidence: 0.667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "data = [\n",
    "    ['milk', 'bread', 'butter'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'bread'],\n",
    "    ['milk', 'bread', 'butter', 'eggs'],\n",
    "    ['bread', 'eggs']\n",
    "]\n",
    "df = pd.DataFrame(data, columns=['Item1', 'Item2', 'Item3', 'Item4']).fillna('')\n",
    "\n",
    "transactions = []\n",
    "for _, row in df.iterrows():\n",
    "    transaction = [item for item in row if item != '']\n",
    "    transactions.append(transaction)\n",
    "\n",
    "def apriori(transactions, min_support=0.4):\n",
    "    itemsets = {}\n",
    "    items = set(item for transaction in transactions for item in transaction)\n",
    "    \n",
    "    # Size-1 itemsets\n",
    "    for item in items:\n",
    "        sup = sum([item in t for t in transactions]) / len(transactions)\n",
    "        if sup >= min_support:\n",
    "            itemsets[frozenset([item])] = sup\n",
    "    \n",
    "    k = 2\n",
    "    current_itemsets = list(itemsets.keys())\n",
    "    \n",
    "    while current_itemsets:\n",
    "        candidates = set()\n",
    "        for i in range(len(current_itemsets)):\n",
    "            for j in range(i + 1, len(current_itemsets)):\n",
    "                union_set = current_itemsets[i] | current_itemsets[j]\n",
    "                if len(union_set) == k:\n",
    "                    candidates.add(union_set)\n",
    "        \n",
    "        valid_itemsets = {}\n",
    "        for candidate in candidates:\n",
    "            sup = sum([candidate.issubset(t) for t in transactions]) / len(transactions)\n",
    "            if sup >= min_support:\n",
    "                valid_itemsets[candidate] = sup\n",
    "        \n",
    "        if not valid_itemsets:\n",
    "            break\n",
    "        \n",
    "        itemsets.update(valid_itemsets)\n",
    "        current_itemsets = list(valid_itemsets.keys())\n",
    "        k += 1\n",
    "    \n",
    "    return itemsets\n",
    "\n",
    "frequent_itemsets = apriori(transactions, min_support=0.4)\n",
    "\n",
    "def generate_rules(frequent_itemsets, min_confidence=0.6):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    antecedent = frozenset(antecedent)\n",
    "                    consequent = itemset - antecedent\n",
    "                    if consequent:\n",
    "                        conf = frequent_itemsets[itemset] / frequent_itemsets[antecedent]\n",
    "                        if conf >= min_confidence:\n",
    "                            rules.append((antecedent, consequent, frequent_itemsets[itemset], conf))\n",
    "    return rules\n",
    "\n",
    "rules = generate_rules(frequent_itemsets)\n",
    "\n",
    "print(\"Top 10 Frequent Itemsets:\")\n",
    "for items, support in sorted(frequent_itemsets.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{set(items)}: {support:.3f}\")\n",
    "\n",
    "print(\"\\nTop 10 Association Rules (sorted by confidence):\")\n",
    "for A, B, support, confidence in sorted(rules, key=lambda x: x[3], reverse=True)[:10]:\n",
    "    print(f\"{set(A)} => {set(B)} | Support: {support:.3f}, Confidence: {confidence:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf76d9-da8a-4fd4-80eb-3916c98ab39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
